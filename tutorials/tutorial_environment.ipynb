{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-Lab Tutorial: Working Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Reinforcement Learning Lab! This is an introductory tutorial for you to familiarize with OpenAI Gym and the first environment for the exercises.\n",
    "\n",
    "## OpenAI Gym environments\n",
    "\n",
    "The environment **Dangerous GridWorld** is visible in the following figure\n",
    "\n",
    "<img src=\"images/environment_1.png\" width=\"400\">\n",
    "Dangerous\n",
    "The agent starts in the cell $0$ and has to reach the cell $19$, while cells $4$, $6$, and $17$ are the dangerous cells that cause the agent to lose the game. \n",
    "The grey cells represent walls that the robot can not cross. \n",
    "The robot can move in $4$ directions: *LEFT*, *RIGHT*, *UP* and *DOWN*. \n",
    "\n",
    "However, the robot **doesn't work very well!** It will follow the commands only the *90%* of the times, in the other *10%* it will perform a random action selecting from the available options, be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the environment we need first to import the packages of OpenAI Gym. \n",
    "Notice that, due to the structure of this repository, we need to add the parent directory to the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path: sys.path.append(module_path)\n",
    "\n",
    "from DangerousGridWorld import GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Than we can generate a new enviromnent **Dangerous GridWorld** and render it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S] [ ] [ ] [ ] [ ] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[W] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [ ] [ ] [ ] [ ] [G] \n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The render is a matrix with cells of different type:\n",
    "* *S* - Start Cell\n",
    "* *W* - Wall Cells\n",
    "* *X* - Death Cells\n",
    "\n",
    "An environment has some useful variables:\n",
    "* *action_space* - number of possible actions (i.e., $4$)]\n",
    "* *observation_space* - space of possible observations (states): usually a range of integers  (i.e., $50$)\n",
    "* *actions* - mapping between action ids and their descriptions\n",
    "* *startstate* - start state (unique)\n",
    "* *goalstate* - goal state (unique)\n",
    "\n",
    "In **Dangerous GridWorld** we have 4 different possible actions numbered from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print( env.action_space )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And they are *Left, Right, Up, Down*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n"
     ]
    }
   ],
   "source": [
    "print( env.actions )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States are numbered from 0 to 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "print( env.observation_space )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some mehtods:\n",
    "* *render()* - renders the environment\n",
    "* *pos_to_state(x, y)* - returns the state id given its position in  and  coordinates\n",
    "* *state_to_pos(state)* - returns the coordinates  given a state id\n",
    "* *is_terminal(state)* - returns True if the given *state* is terminal (goal or death)\n",
    "* *evaluate_policy(policy)* - return the average cumulative reward of 10 runs following the given policy\n",
    "* *render_policy()* - renders the policy, showing the selected action for each cell \n",
    "\n",
    "For example, if we want to know the ids and positions for both the start and goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start id: 0\tGoal id: 48\n",
      "Start position: (0, 0)\tGoal position: (6, 6)\n",
      "Id of state (0, 3): 3\n",
      "\n",
      "[S] [ ] [ ] [ ] [ ] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[W] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [W] [W] [X] [ ] [X] \n",
      "[ ] [W] [W] [W] [X] [ ] [X] \n",
      "[ ] [ ] [ ] [ ] [ ] [ ] [G] \n"
     ]
    }
   ],
   "source": [
    "start = env.start_state\n",
    "goal = env.goal_state\n",
    "print(\"Start id: {}\\tGoal id: {}\".format(start, goal))\n",
    "print(\"Start position: {}\\tGoal position: {}\".format(env.state_to_pos(start), env.state_to_pos(goal)))\n",
    "print(\"Id of state (0, 3): {}\".format(env.pos_to_state(3, 0)))\n",
    "print()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, it can be necessary to know if a state is **terminal**, in general, the **goal state** and the death states are terminal. Using a the *is_terminal(state)* function is a fast method to obtain this information. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state 1 ((1, 0)), is terminal? False\n",
      "The state 6 ((6, 0)), is terminal? True\n",
      "The state 48 ((6, 6)), is terminal? True\n"
     ]
    }
   ],
   "source": [
    "is_1_terminal = env.is_terminal(1)\n",
    "is_6_terminal = env.is_terminal(6)\n",
    "is_48_terminal = env.is_terminal(48)\n",
    "\n",
    "print( f\"The state 1 ({env.state_to_pos(1)}), is terminal? {is_1_terminal}\" )\n",
    "print( f\"The state 6 ({env.state_to_pos(6)}), is terminal? {is_6_terminal}\" )\n",
    "print( f\"The state 48 ({env.state_to_pos(48)}), is terminal? {is_48_terminal}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Methods: *sample()* vs *transition_prob()*\n",
    "\n",
    "In **Dangerous GridWorld** there are two key methods to navigate the environment:\n",
    "* *sample(state, action)* - returns a new state sampled from the ones that can be reached from *state* by performing *action* both given as ids\n",
    "* *transition_prob(state, action, next_state)* - returns the probability of reaching the state *next_state*, starting from *state* and selecting the action *action*\n",
    "\n",
    "In some cases, we want to analyze only the transition table (e.g., policy/value iteration) so we can use the function **transition_prob** to obtain the probability of reaching a state, in some other cases we want to actually move the agent in the environment (e.g., MC tree-search or testing phase) and we use the function **sample** to try to execute the action and see what happen *(remember, the robot will follow your instructions only 90% of the time!)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an example of the method **transition_prob(state, action, new_state)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting D (DOWN)? 0.9\n",
      "What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting R (RIGHT)? 0.1\n",
      "What's the probability of ending up in state 48 (6, 6) starting from state 0 (0, 0) and selecting R (RIGHT)? 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\"What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting D (DOWN)? {env.transition_prob(0, 3, 7)}\" )\n",
    "print( f\"What's the probability of ending up in state  7 (0, 1) starting from state 0 (0, 0) and selecting R (RIGHT)? {env.transition_prob(0, 1, 7)}\" )\n",
    "print( f\"What's the probability of ending up in state 48 (6, 6) starting from state 0 (0, 0) and selecting R (RIGHT)? {env.transition_prob(0, 1, 48)}\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following an example of the method **sample**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (0, 1)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (0, 1)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (0, 1)\n",
      "Starting from (0, 0) and performing action L, the reobot ends up in state: (1, 0)\n"
     ]
    }
   ],
   "source": [
    "start_position = 0\n",
    "action = 0\n",
    "\n",
    "for _ in range(5):\n",
    "    new_state = env.sample(start_position, 0) \n",
    "    print( f\"Starting from {env.state_to_pos(start_position)} and performing action {env.actions[0]}, the reobot ends up in state: {env.state_to_pos(new_state)}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b4a0bc6cf61cb63657bef9bf4f66287d0630dbbd28c8eb6c57e9ede2e775d87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
